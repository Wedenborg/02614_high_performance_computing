Loaded module: cuda/10.2
Loaded dependency [gcc/6.3.0]: binutils/2.29
Loaded module: gcc/6.3.0
==104293== NVPROF is profiling process 104293, command: ./matmult_f.nvcc gpulib 500 500 500
==104293== Profiling application: ./matmult_f.nvcc gpulib 500 500 500
  5859.375 113369.326 # matmult_gpulib
==104293== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   57.62%  449.91ms      4084  110.16us  1.1200us  165.79us  [CUDA memcpy HtoD]
                   26.79%  209.18ms      1361  153.69us  153.15us  161.12us  [CUDA memcpy DtoH]
                   15.60%  121.77ms      1361  89.474us  85.727us  100.67us  void gemm_kernel2x2_core<double, bool=0, bool=0, bool=0, bool=0, bool=0>(double*, double const *, double const *, int, int, int, int, int, int, double*, double*, double, double, int)
==104311== NVPROF is profiling process 104311, command: ./matmult_f.nvcc gpulib 1000 1000 1000
==104311== Profiling application: ./matmult_f.nvcc gpulib 1000 1000 1000
 23437.500 435555.143 # matmult_gpulib
==104311== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   56.47%  852.59ms      1963  434.33us  1.4400us  651.77us  [CUDA memcpy HtoD]
                   26.40%  398.54ms       654  609.39us  608.96us  620.70us  [CUDA memcpy DtoH]
                   17.08%  257.86ms       654  394.28us  392.80us  400.06us  volta_dgemm_64x64_nn
                    0.06%  904.67us       654  1.3830us  1.3440us  1.7280us  [CUDA memset]
==104328== NVPROF is profiling process 104328, command: ./matmult_f.nvcc gpulib 2000 2000 2000
==104328== Profiling application: ./matmult_f.nvcc gpulib 2000 2000 2000
 93750.000 1062276.293 # matmult_gpulib
==104328== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   48.99%  1.03875s       601  1.7284ms  1.4400us  2.6066ms  [CUDA memcpy HtoD]
                   28.08%  595.32ms       200  2.9766ms  2.6372ms  2.9974ms  volta_dgemm_64x64_nn
                   22.93%  486.17ms       200  2.4309ms  2.4305ms  2.4408ms  [CUDA memcpy DtoH]
==104346== NVPROF is profiling process 104346, command: ./matmult_f.nvcc gpulib 4000 4000 4000
==104346== Profiling application: ./matmult_f.nvcc gpulib 4000 4000 4000
==104346== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   48.19%  1.32426s       148  8.9477ms  1.4400us  16.911ms  [CUDA memcpy HtoD]
                   34.48%  947.58ms        49  19.338ms  19.315ms  19.355ms  volta_dgemm_128x64_nn
                   17.33%  476.25ms        49  9.7193ms  9.7188ms  9.7249ms  [CUDA memcpy DtoH]
375000.000 2060982.292 # matmult_gpulib
==104364== NVPROF is profiling process 104364, command: ./matmult_f.nvcc gpulib 8000 8000 8000
==104364== Profiling application: ./matmult_f.nvcc gpulib 8000 8000 8000
==104364== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   48.74%  1.47131s        10  147.13ms  146.75ms  147.41ms  volta_dgemm_128x64_nn
                   38.38%  1.15871s        31  37.378ms  1.4400us  78.735ms  [CUDA memcpy HtoD]
                   12.88%  388.71ms        10  38.871ms  38.870ms  38.880ms  [CUDA memcpy DtoH]
1500000.000 3290566.595 # matmult_gpulib

------------------------------------------------------------
Sender: LSF System <lsfadmin@n-62-20-13>
Subject: Job 5173477: <mm_batch> in cluster <dcc> Done

Job <mm_batch> was submitted from host <n-62-20-6> by user <s164548> in cluster <dcc> at Thu Jan 23 19:27:23 2020
Job was executed on host(s) <n-62-20-13>, in queue <hpcintrogpu>, as user <s164548> in cluster <dcc> at Thu Jan 23 19:27:24 2020
</zhome/1c/9/118567> was used as the home directory.
</zhome/1c/9/118567/Matmult> was used as the working directory.
Started at Thu Jan 23 19:27:24 2020
Terminated at Thu Jan 23 19:27:49 2020
Results reported at Thu Jan 23 19:27:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
# 02614 - High-Performance Computing, January 2018
# 
# batch script to run matmult on a decidated server in the hpcintro
# queue
#
# Author: Bernd Dammann <bd@cc.dtu.dk>
#
#BSUB -J mm_batch
#BSUB -o mm_batch_%J.out
#BSUB -q hpcintrogpu
#BSUB -n 1
#BSUB -gpu "num=1:mode=exclusive_process:mps=yes"
#BSUB -R "rusage[mem=7GB]"
#BSUB -W 5

module load cuda/10.2
module load gcc/6.3.0

# define the driver name to use
# valid values: matmult_c.studio, matmult_f.studio, matmult_c.gcc or
# matmult_f.gcc
#
EXECUTABLE=matmult_f.nvcc

# define the mkn values in the MKN variable
#
SIZES="500 1000 2000 4000 8000"

# define the permutation type in PERM
#
PERM="gpulib"

# uncomment and set a reasonable BLKSIZE for the blk version
#
# BLKSIZE=1

# enable(1)/disable(0) result checking
export MATMULT_COMPARE=0

# start the collect command with the above settings
for S in $SIZES
do
    MATMULT_COMPARE=0 nvprof --print-gpu-summary ./$EXECUTABLE $PERM $S $S $S
    #ATMULT_COMPARE=0 ./$EXECUTABLE $PERM $S $S $S
done

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   23.00 sec.
    Max Memory :                                 416 MB
    Average Memory :                             416.00 MB
    Total Requested Memory :                     7168.00 MB
    Delta Memory :                               6752.00 MB
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                15
    Run time :                                   26 sec.
    Turnaround time :                            26 sec.

The output (if any) is above this job summary.

